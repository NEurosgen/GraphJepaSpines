"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ GraphJEPA
–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Å –ø–æ–º–æ—â—å—é PCA, t-SNE –∏ UMAP.
"""

import torch
import torch.serialization
import numpy as np
from tqdm import tqdm
import os
import matplotlib.pyplot as plt
import seaborn as sns

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è –º–µ—Ç–æ–¥–æ–≤ —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
try:
    import umap
except ImportError:
    print("\n[WARNING] UMAP library not found. Install with `pip install umap-learn`.")
    umap = None

# PyTorch 2.6+ —Ç—Ä–µ–±—É–µ—Ç —è–≤–Ω–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –¥–ª—è OmegaConf –∫–ª–∞—Å—Å–æ–≤
from omegaconf import DictConfig, ListConfig
torch.serialization.add_safe_globals([DictConfig, ListConfig])

from src.data_utils.transforms import GenNormalize, FeatureChoice, NormNoEps, EdgeNorm
from src.data_utils.datamodule import GraphDataModule, GraphDataSet
# from src.models.jepa import JepaLight # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é –≤ —ç—Ç–æ–º —Å–∫—Ä–∏–ø—Ç–µ


def load_stats(path: str):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —É–∑–ª–æ–≤ –∏ —Ä—ë–±–µ—Ä."""
    mean_x = torch.load(path + "means.pt")
    std_x = torch.load(path + "stds.pt")
    mean_edge = torch.load(path + "mean_edge.pt")
    std_edge = torch.load(path + "std_edge.pt")
    return mean_x, std_x, mean_edge, std_edge


def get_datamodule(path: str, stats_path: str, batch_size: int = 1, features: list = None):
    """–°–æ–∑–¥–∞—ë—Ç DataModule –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –≥—Ä–∞—Ñ–æ–≤.
    
    Args:
        path: –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É
        stats_path: –ü—É—Ç—å –∫ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        features: –°–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤ —Ñ–∏—á –¥–ª—è –≤—ã–±–æ—Ä–∞ (–µ—Å–ª–∏ None - –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤—Å–µ)
    """
    mean_x, std_x, mean_edge, std_edge = load_stats(stats_path)
    
    # –°–æ–∑–¥–∞—ë–º pipeline —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π
    transforms = []
    
    if features is not None:
        # –°–Ω–∞—á–∞–ª–∞ –≤—ã–±–∏—Ä–∞–µ–º –Ω—É–∂–Ω—ã–µ —Ñ–∏—á–∏
        transforms.append(FeatureChoice(features))
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–æ–ª—å–∫–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–µ —Ñ–∏—á–∏ (—Å—Ä–µ–∑–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏)
        mean_x = mean_x[features]
        std_x = std_x[features]
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é
    transforms.append(NormNoEps(mean_x, std_x))
    transforms.append(EdgeNorm(mean_edge, std_edge))
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤ GenNormalize (–±–µ–∑ mask_transform –¥–ª—è inference)
    norm = GenNormalize(transforms=transforms, mask_transform=None)
    
    ds = GraphDataSet(path=path, transform=norm)
    
    # Collate function –¥–ª—è PyG Data –æ–±—ä–µ–∫—Ç–æ–≤
    from torch_geometric.data import Batch
    def collate_fn(data_list):
        return Batch.from_data_list(data_list)
    
    datamodule = GraphDataModule(
        ds, 
        batch_size,
        num_workers=0,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º 0 –¥–ª—è inference
        seed=42,
        ratio=[1, 0, 0],  # –í—Å–µ –¥–∞–Ω–Ω—ã–µ –≤ train split –¥–ª—è inference
        collate_fn=collate_fn
    )
    return datamodule


def extract_embeddings(encoder, datamodule: GraphDataModule, 
                       label: int, sigma: float = 1.0, device: str = 'cuda'):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É—è –æ–±—É—á–µ–Ω–Ω—ã–π —ç–Ω–∫–æ–¥–µ—Ä.
    """
    encoder = encoder.to(device)
    encoder.eval()
    
    embeddings_list = []
    labels_list = []
    filenames_list = []
    
    datamodule.setup("fit")
    dataloader = datamodule.train_dataloader()
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc=f"Extracting embeddings (class {label})"):
            # Batch –º–æ–∂–µ—Ç –±—ã—Ç—å tuple (context, target) –∏–ª–∏ –æ–¥–∏–Ω –≥—Ä–∞—Ñ
            if isinstance(batch, tuple):
                context_batch, _ = batch
            else:
                context_batch = batch
            
            context_batch = context_batch.to(device)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º RBF –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∫ edge_attr (–∫–∞–∫ –≤ training_step)
            edge_attr = context_batch.edge_attr
            if edge_attr is not None:
                edge_attr = torch.exp(-edge_attr**2 / sigma**2)
            
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ —ç–Ω–∫–æ–¥–µ—Ä
            emb = encoder(
                context_batch.x, 
                context_batch.edge_index, 
                edge_attr
            )
            
            # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —É–∑–ª–æ–≤ –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥ –≥—Ä–∞—Ñ–∞ (mean pooling)
            if hasattr(context_batch, 'batch') and context_batch.batch is not None:
                # –ù–µ—Å–∫–æ–ª—å–∫–æ –≥—Ä–∞—Ñ–æ–≤ –≤ –±–∞—Ç—á–µ
                from torch_geometric.nn import global_mean_pool
                graph_emb = global_mean_pool(emb, context_batch.batch)
            else:
                # –û–¥–∏–Ω –≥—Ä–∞—Ñ
                graph_emb = emb.mean(dim=0, keepdim=True)
            
            embeddings_list.append(graph_emb.cpu())
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∫–∏
            batch_size = graph_emb.size(0)
            labels_list.extend([label] * batch_size)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤ –µ—Å–ª–∏ –µ—Å—Ç—å
            if hasattr(context_batch, 'file_name'):
                if isinstance(context_batch.file_name, list):
                    filenames_list.extend(context_batch.file_name)
                else:
                    filenames_list.append(context_batch.file_name)
            else:
                filenames_list.extend([f"graph_{i}" for i in range(batch_size)])
    
    embeddings = torch.cat(embeddings_list, dim=0)
    labels = np.array(labels_list)
    
    return embeddings, labels, filenames_list


# --- –ù–û–í–´–ô –ë–õ–û–ö –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ò ---

def plot_scatter(X_2d, labels, title, save_path):
    """
    –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç—Ä–∏—Å–æ–≤–∫–∏ 2D –≥—Ä–∞—Ñ–∏–∫–∞ —Ä–∞—Å—Å–µ—è–Ω–∏—è.
    """
    plt.figure(figsize=(11, 9))
    sns.set_theme(style="whitegrid")
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —á–∏—Å–ª–æ–≤—ã–µ –º–µ—Ç–∫–∏ –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –¥–ª—è –ª–µ–≥–µ–Ω–¥—ã
    label_names = ['AB (class 0)' if l == 0 else 'WT (class 1)' for l in labels]
    
    scatter = sns.scatterplot(
        x=X_2d[:, 0], 
        y=X_2d[:, 1],
        hue=label_names,
        palette=sns.color_palette("deep", len(np.unique(labels))),
        style=label_names,
        s=60,
        alpha=0.8,
        edgecolor='w'
    )
    
    plt.title(title, fontsize=16, fontweight='bold', pad=20)
    plt.xlabel("Dimension 1", fontsize=12)
    plt.ylabel("Dimension 2", fontsize=12)
    plt.legend(title="Classes", title_fontsize=12, fontsize=11)
    
    # –£–±–∏—Ä–∞–µ–º —Ä–∞–º–∫–∏ —Å–≤–µ—Ä—Ö—É –∏ —Å–ø—Ä–∞–≤–∞ –¥–ª—è —á–∏—Å—Ç–æ—Ç—ã
    sns.despine()
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"   ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω –≥—Ä–∞—Ñ–∏–∫: {save_path}")


def visualize_embeddings(embeddings: torch.Tensor, labels: np.ndarray, output_dir: str,tag = ''):
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç PCA, t-SNE –∏ UMAP –ø—Ä–æ–µ–∫—Ü–∏–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≥—Ä–∞—Ñ–∏–∫–∏.
    """
    print("\n" + "="*60)
    print("–í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –õ–ê–¢–ï–ù–¢–ù–û–ì–û –ü–†–û–°–¢–†–ê–ù–°–¢–í–ê")
    print("="*60)
    
    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    os.makedirs(output_dir, exist_ok=True)
    
    # –ü–µ—Ä–µ–≤–æ–¥–∏–º –≤ numpy –¥–ª—è sklearn/umap
    X = embeddings.numpy()
    
    # 1. PCA (Principal Component Analysis) - –õ–∏–Ω–µ–π–Ω—ã–π –º–µ—Ç–æ–¥
    print("\nüìä –í—ã—á–∏—Å–ª–µ–Ω–∏–µ PCA (2 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã)...")
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X)
    explained_variance = pca.explained_variance_ratio_
    title_pca = f"PCA Projection (Explained Variance: {explained_variance[0]+explained_variance[1]:.2%})"
    plot_scatter(X_pca, labels, title_pca, os.path.join(output_dir, tag + "visualization_pca.png"))
    
    # 2. t-SNE (t-distributed Stochastic Neighbor Embedding) - –ù–µ–ª–∏–Ω–µ–π–Ω—ã–π, –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–π
    print("\nüó∫Ô∏è –í—ã—á–∏—Å–ª–µ–Ω–∏–µ t-SNE...")
    # –ü–∞—Ä–∞–º–µ—Ç—Ä perplexity –≤–ª–∏—è–µ—Ç –Ω–∞ –±–∞–ª–∞–Ω—Å –≤–Ω–∏–º–∞–Ω–∏—è –º–µ–∂–¥—É –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –∏ –≥–ª–æ–±–∞–ª—å–Ω—ã–º–∏ –∞—Å–ø–µ–∫—Ç–∞–º–∏
    # –û–±—ã—á–Ω–æ –≤—ã–±–∏—Ä–∞—é—Ç –º–µ–∂–¥—É 5 –∏ 50. –ß–µ–º –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö, —Ç–µ–º –±–æ–ª—å—à–µ –º–æ–∂–Ω–æ —Å—Ç–∞–≤–∏—Ç—å.
    tsne = TSNE(n_components=2, perplexity=min(30, len(X)/10), max_iter=1500, random_state=42, n_jobs=-1)
    X_tsne = tsne.fit_transform(X)
    plot_scatter(X_tsne, labels, "t-SNE Projection", os.path.join(output_dir,tag + "visualization_tsne.png"))
    
    # 3. UMAP (Uniform Manifold Approximation and Projection) - –ù–µ–ª–∏–Ω–µ–π–Ω—ã–π, —Ç–æ–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π
    # –û–±—ã—á–Ω–æ –±—ã—Å—Ç—Ä–µ–µ t-SNE –∏ –ª—É—á—à–µ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
    if umap is not None:
        print("\nüåå –í—ã—á–∏—Å–ª–µ–Ω–∏–µ UMAP...")
        # n_neighbors: –±–∞–ª–∞–Ω—Å –ª–æ–∫–∞–ª—å–Ω–æ–π (–º–µ–Ω—å—à–µ) –∏ –≥–ª–æ–±–∞–ª—å–Ω–æ–π (–±–æ–ª—å—à–µ) —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (default=15)
        # min_dist: –Ω–∞—Å–∫–æ–ª—å–∫–æ –ø–ª–æ—Ç–Ω–æ –º–æ–≥—É—Ç –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å—Å—è —Ç–æ—á–∫–∏ (default=0.1)
        umap_reducer = umap.UMAP(n_components=2, n_neighbors=20, min_dist=0.1, random_state=42, n_jobs=-1)
        X_umap = umap_reducer.fit_transform(X)
        plot_scatter(X_umap, labels, "UMAP Projection", os.path.join(output_dir,tag + "visualization_umap.png"))
    else:
        print("   –ü—Ä–æ–ø—É—Å–∫ UMAP (–±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞).")

# --- –ö–û–ù–ï–¶ –ù–û–í–û–ì–û –ë–õ–û–ö–ê ---


def main():
    # –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º
    stats_path = '/home/eugen/Desktop/CodeWork/Projects/Diplom/notebooks/GIT_Graph_refactor/data/stats_9009/'
    path_ab = "/home/eugen/Desktop/CodeWork/Projects/Diplom/notebooks/notebooks/graph_dataset_output_ab"
    path_wt = "/home/eugen/Desktop/CodeWork/Projects/Diplom/notebooks/notebooks/graph_dataset_output_wt"
    checkpoint_path = "/home/eugen/Desktop/CodeWork/Projects/Diplom/notebooks/GIT_Graph_refactor/lightning_logs/version_142/checkpoints/epoch=62-step=139923.ckpt"
    
    # –§–∏—á–∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π –æ–±—É—á–µ–Ω–Ω—ã—Ö —Å FeatureChoice (–∏–∑ main –≤–µ—Ç–∫–∏)
    # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ —Å–æ –≤—Å–µ–º–∏ —Ñ–∏—á–∞–º–∏ - —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å features = None
    features = [0, 4, 5, 6, 7, 13, 14, 15, 17, 19, 20]
    
    # –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    tag = "jepa_feature_"

    output_base_path = '/home/eugen/Desktop/CodeWork/Projects/Diplom/notebooks/GIT_Graph_refactor/exp/'
    visualization_dir = os.path.join(output_base_path, "visualizations")

    # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Using device: {device}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç –Ω–∞–ø—Ä—è–º—É—é
    print("\nüì¶ –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞...")
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    
    # –°–º–æ—Ç—Ä–∏–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É state_dict —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —ç–Ω–∫–æ–¥–µ—Ä–∞
    state_dict = checkpoint['state_dict']
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å —ç–Ω–∫–æ–¥–µ—Ä–∞ (—Ä–∞–∑–Ω—ã–µ –¥–ª—è LeJEPA –∏ GraphJepa)
    encoder_prefix = None
    for key in state_dict.keys():
        if 'encoder.proj.weight' in key:
            if 'student_encoder' in key:
                encoder_prefix = 'model.student_encoder.'
            else:
                encoder_prefix = 'model.encoder.'
            break
    
    if encoder_prefix is None:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —ç–Ω–∫–æ–¥–µ—Ä –≤ —á–µ–∫–ø–æ–∏–Ω—Ç–µ")
    
    # –ù–∞—Ö–æ–¥–∏–º proj.weight —á—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
    proj_key = f'{encoder_prefix}proj.weight'
    proj_weight = state_dict[proj_key]
    out_channels, in_channels = proj_weight.shape
    
    # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤
    num_layers = 0
    for key in state_dict.keys():
        if f'{encoder_prefix}layers' in key and '.model' in key:
            layer_idx = int(key.split('layers.')[1].split('.')[0])
            num_layers = max(num_layers, layer_idx + 1)
    
    print(f"   –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ: in={in_channels}, out={out_channels}, layers={num_layers}")
    print(f"   –ü—Ä–µ—Ñ–∏–∫—Å —ç–Ω–∫–æ–¥–µ—Ä–∞: {encoder_prefix}")
    
    # –°–æ–∑–¥–∞—ë–º —ç–Ω–∫–æ–¥–µ—Ä
    # (–≠—Ç–æ—Ç –∏–º–ø–æ—Ä—Ç –¥–æ–ª–∂–µ–Ω —Ä–∞–±–æ—Ç–∞—Ç—å, –µ—Å–ª–∏ –≤–∞—à –ø—Ä–æ–µ–∫—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω —Ç–∞–∫ –∂–µ, –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
    try:
        from src.models.encoder import GraphGcnEncoder
    except ImportError:
         print("\n–û–®–ò–ë–ö–ê –ò–ú–ü–û–†–¢–ê: –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Å–∫—Ä–∏–ø—Ç –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –∏–∑ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞, —á—Ç–æ–±—ã 'src' –±—ã–ª –¥–æ—Å—Ç—É–ø–µ–Ω.")
         exit(1)

    encoder = GraphGcnEncoder(
        in_channels=in_channels,
        out_channels=out_channels,
        num_layers=num_layers
    )
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–µ—Å–∞ —ç–Ω–∫–æ–¥–µ—Ä–∞
    encoder_state_dict = {}
    for key, value in state_dict.items():
        if key.startswith(encoder_prefix):
            new_key = key[len(encoder_prefix):]
            encoder_state_dict[new_key] = value
    
    encoder.load_state_dict(encoder_state_dict)
    encoder = encoder.to(device)
    encoder.eval()
    print("‚úÖ –≠–Ω–∫–æ–¥–µ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
    
    # –°–æ–∑–¥–∞—ë–º DataModules –¥–ª—è –æ–±–æ–∏—Ö –∫–ª–∞—Å—Å–æ–≤
    print("\nüìÇ –°–æ–∑–¥–∞–Ω–∏–µ DataModules...")
    dm_ab = get_datamodule(path_ab, stats_path, batch_size=32, features=features)
    dm_wt = get_datamodule(path_wt, stats_path, batch_size=32, features=features)
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    print("\nüîÑ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
    embeddings_ab, labels_ab, files_ab = extract_embeddings(
        encoder, dm_ab, label=0, device=device
    )
    embeddings_wt, labels_wt, files_wt = extract_embeddings(
        encoder, dm_wt, label=1, device=device
    )
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –æ–±–æ–∏—Ö –∫–ª–∞—Å—Å–æ–≤
    all_embeddings = torch.cat([embeddings_ab, embeddings_wt], dim=0)
    all_labels = np.concatenate([labels_ab, labels_wt])
    all_files = files_ab + files_wt
    
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞:")
    print(f"   –ö–ª–∞—Å—Å AB (label=0): {len(labels_ab)} –≥—Ä–∞—Ñ–æ–≤")
    print(f"   –ö–ª–∞—Å—Å WT (label=1): {len(labels_wt)} –≥—Ä–∞—Ñ–æ–≤")
    print(f"   –í—Å–µ–≥–æ: {len(all_labels)} –≥—Ä–∞—Ñ–æ–≤")
    print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {all_embeddings.shape[1]}")
    
    # --- –ó–ê–ü–£–°–ö –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ò –í–ú–ï–°–¢–û –≠–°–¢–ò–ú–ê–¢–û–†–û–í ---
    visualize_embeddings(all_embeddings, all_labels, visualization_dir,tag = tag)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∞–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –º–µ—Ç–∫–∏ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—ã—Ä—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ {output_base_path}...")
    torch.save({
        'embeddings': all_embeddings,
        'labels': all_labels,
        'files': all_files
    }, os.path.join(visualization_dir, tag + 'embeddings_raw.pt'))
    
    print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ! –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {visualization_dir}")
    
    return all_embeddings, all_labels


if __name__ == "__main__":
    main()