import os
import json
import copy
import hydra  # <--- –í–ê–ñ–ù–û
from pathlib import Path
from datetime import datetime
from typing import Any, Dict, List, Optional

import torch
import numpy as np
import pytorch_lightning as L
from pytorch_lightning.callbacks import Callback
from omegaconf import OmegaConf

# –í–∞—à–∏ –∏–º–ø–æ—Ä—Ç—ã
from src.models.jepa import JepaLight
from src.data_utils.transforms import (
    GenNormalize, 
    create_mask_collate_fn,
    NormNoEps,
    EdgeNorm,
    GraphPruning,
    MaskData,
    FeatureChoice
)
from src.data_utils.datamodule import GraphDataSet, GraphDataModule

torch.set_float32_matmul_precision('high')


# ==========================================
# 1. –£–¢–ò–õ–ò–¢–´ –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò
# ==========================================

def load_config(path: str):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç YAML –≤ –æ–±—ä–µ–∫—Ç OmegaConf."""
    if not os.path.exists(path):
        raise FileNotFoundError(f"Config not found: {path}")
    conf = OmegaConf.load(path)
    return conf 

def update_config_by_path(config, path: str, value: Any):
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç OmegaConf –æ–±—ä–µ–∫—Ç –ø–æ –ø—É—Ç–∏.
    OmegaConf.update(config, path, value) - –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–π —Å–ø–æ—Å–æ–±
    """
    OmegaConf.update(config, path, value)


def load_stats(path: str):
    return (
        torch.load(path + "means.pt"),
        torch.load(path + "stds.pt"),
        torch.load(path + "mean_edge.pt"),
        torch.load(path + "std_edge.pt")
    )


def build_transforms(cfg, mean_x, std_x, mean_edge, std_edge):
    """
    Build transforms by config
    """
    transforms = []
    
    features = cfg.get('features', None)
    if features is not None:
        features = list(features)
        transforms.append(FeatureChoice(feature=features))
        mean_x = mean_x[features]
        std_x = std_x[features]
    
    transforms.append(NormNoEps(mean=mean_x, std=std_x, eps=cfg.get('eps', 1e-6)))
    transforms.append(EdgeNorm(mean=mean_edge, std=std_edge))
    
    knn_k = cfg.get('knn', -1)
    if knn_k > 0:
        transforms.append(GraphPruning(k=knn_k, mutual=cfg.get('mutual_knn', False)))
    
    return transforms


def get_datamodule(cfg):
    mean_x, std_x, mean_edge, std_edge = load_stats(cfg.dataset.stats_path)
    
    transforms = build_transforms(cfg, mean_x, std_x, mean_edge, std_edge)
    mask_transform = MaskData(mask_ratio=cfg.mask_ratio)
    gen_normalize = GenNormalize(transforms=transforms, mask_transform=mask_transform)
    
    collate_fn = create_mask_collate_fn(gen_normalize)
    ds = GraphDataSet(path=cfg.dataset.path, transform=None)

    datamodule = GraphDataModule(
        ds, 
        cfg.batch_size,
        num_workers=cfg.num_workers, 
        seed=cfg.seed,
        ratio=cfg.ratio,
        collate_fn=collate_fn
    )
    return datamodule


def create_repr_dataloader(repr_cfg):
    """
    –°–æ–∑–¥–∞–µ—Ç dataloader –¥–ª—è –æ—Ü–µ–Ω–∫–∏ representation quality.
    
    Returns:
        Tuple[DataLoader, np.ndarray]: DataLoader –∏ –º–∞—Å—Å–∏–≤ –º–µ—Ç–æ–∫
    """
    from torch.utils.data import DataLoader, ConcatDataset
    from torch_geometric.data import Batch
    
    mean_x, std_x, mean_edge, std_edge = load_stats(repr_cfg.stats_path)
    transforms = build_transforms(repr_cfg, mean_x, std_x, mean_edge, std_edge)
    norm = GenNormalize(transforms=transforms, mask_transform=None)
    
    datasets = []
    labels = []
    
    for ds_cfg in repr_cfg.datasets:
        ds = GraphDataSet(path=ds_cfg.path, transform=norm)
        datasets.append(ds)
        labels.extend([ds_cfg.label] * len(ds))

    combined_dataset = ConcatDataset(datasets)
    labels_array = np.array(labels)
    
    def collate_fn(data_list):
        return Batch.from_data_list(data_list)
    
    dataloader = DataLoader(
        combined_dataset,
        batch_size=repr_cfg.batch_size,
        shuffle=False,
        num_workers=0,
        collate_fn=collate_fn
    )
    
    return dataloader, labels_array


def create_components(cfg: OmegaConf, seed: int):
    """
    –°–æ–∑–¥–∞–µ—Ç –æ–±—ä–µ–∫—Ç—ã, –∏—Å–ø–æ–ª—å–∑—É—è Hydra Instantiation.
    """
    model = hydra.utils.instantiate(cfg.network)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É repr_kwargs –¥–ª—è —ç—Å—Ç–∏–º–∞—Ç–æ—Ä–æ–≤
    repr_kwargs = {}
    if cfg.get('representation', {}).get('enabled', False):
        repr_cfg = cfg.representation
        repr_dl, repr_labels = create_repr_dataloader(repr_cfg)
        repr_kwargs = {
            'repr_dl': repr_dl,
            'repr_labels': repr_labels,
            'estimator_cfg': {'estimators': list(repr_cfg.estimators)}
        }
    
    lightning_module = JepaLight(model=model, cfg=cfg.training, debug=False, **repr_kwargs)
    datamodule = get_datamodule(cfg.datamodule)
    
    return lightning_module, datamodule


# ==========================================
# 3. –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢
# ==========================================

class ExperimentLogger:
    def __init__(self, output_dir: str, experiment_name: str, base_config: OmegaConf, param_name: str):
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.filepath = os.path.join(output_dir, f"{experiment_name}_{timestamp}.json")
        
        config_dict = OmegaConf.to_container(base_config, resolve=True)
        
        self.data = {
            "meta": { "timestamp": timestamp, "variable_param": param_name, "base_config": config_dict },
            "results": []
        }
        self._initial_save()

    def _initial_save(self):
        with open(self.filepath, "w", encoding="utf-8") as f:
            json.dump(self.data, f, indent=2, ensure_ascii=False)

    def log_result(self, result: dict):
        self.data["results"].append(result)
        with open(self.filepath, "w", encoding="utf-8") as f:
            json.dump(self.data, f, indent=2, ensure_ascii=False)


class MetricsTracker(Callback):
    """Callback –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫."""
    
    def __init__(self, metrics_to_track: List[str] = None):
        super().__init__()
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ val_loss
        self.metrics_to_track = metrics_to_track or ['val_loss']
        self.history: Dict[str, List[float]] = {m: [] for m in self.metrics_to_track}
        # –ú–µ—Ç—Ä–∏–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –º–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å (–æ—Å—Ç–∞–ª—å–Ω—ã–µ –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É—é—Ç—Å—è)
        self.minimize_metrics = {'val_loss', 'train_loss', 'alignment', 'davies_bouldin', 'intra_class_distance'}
        
    def on_validation_epoch_end(self, trainer, pl_module):
        for metric_name in self.metrics_to_track:
            value = trainer.callback_metrics.get(metric_name)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º repr/ (–º–µ—Ç—Ä–∏–∫–∏ —ç—Å—Ç–∏–º–∞—Ç–æ—Ä–æ–≤)
            if value is None:
                value = trainer.callback_metrics.get(f"repr/{metric_name}")
            if value is not None:
                val = value.item() if hasattr(value, 'item') else value
                self.history[metric_name].append(val)
    
    def get_best(self, metric_name: str) -> Optional[float]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ª—É—á—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ (min –∏–ª–∏ max –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞)."""
        values = self.history.get(metric_name, [])
        if not values:
            return None
        if metric_name in self.minimize_metrics:
            return min(values)
        return max(values)
    
    def get_last(self, metric_name: str) -> Optional[float]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏."""
        values = self.history.get(metric_name, [])
        return values[-1] if values else None
    
    def get_all(self, metric_name: str) -> List[float]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å—é –∏—Å—Ç–æ—Ä–∏—é –º–µ—Ç—Ä–∏–∫–∏."""
        return self.history.get(metric_name, [])


def run_single_trial(config, seed, param_info, metrics_to_track: List[str] = None):
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–¥–∏–Ω —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
    
    Args:
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
        seed: Random seed
        param_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–∞—Ä—å–∏—Ä—É–µ–º–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–µ
        metrics_to_track: –°–ø–∏—Å–æ–∫ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ['val_loss'])
    
    Returns:
        dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ —Å best_* –∏ last_* –¥–ª—è –∫–∞–∂–¥–æ–π –º–µ—Ç—Ä–∏–∫–∏
    """
    if metrics_to_track is None:
        metrics_to_track = ['val_loss']
    
    L.seed_everything(seed, workers=True)
    
    model_module, datamodule = create_components(config, seed)
    metrics_tracker = MetricsTracker(metrics_to_track=metrics_to_track)
    
    trainer = L.Trainer(
        max_epochs=config.training.max_epochs,
        accelerator="gpu",
        devices=1,
        log_every_n_steps=10,
        enable_progress_bar=True,
        enable_model_summary=False,
        callbacks=[metrics_tracker],
        logger=False
    )

    trainer.fit(model_module, datamodule=datamodule)
    
    # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –≤—Å–µ–º –º–µ—Ç—Ä–∏–∫–∞–º
    result = {
        "param_value": param_info["value"],
        "param_name": param_info["name"],
        "seed": seed,
    }
    
    for metric in metrics_to_track:
        result[f"best_{metric}"] = metrics_tracker.get_best(metric)
        result[f"last_{metric}"] = metrics_tracker.get_last(metric)
    
    # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –¥—É–±–ª–∏—Ä—É–µ–º val_loss
    if 'val_loss' in metrics_to_track:
        result["min_val_loss"] = result.get("best_val_loss")
        result["last_val_loss"] = result.get("last_val_loss")
    
    return result


def run_experiment_grid(
    param_to_vary: str, 
    values: list, 
    seeds: list, 
    metrics_to_track: List[str] = None
):
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç grid search —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç.
    
    Args:
        param_to_vary: –ü—É—Ç—å –∫ –ø–∞—Ä–∞–º–µ—Ç—Ä—É –¥–ª—è –≤–∞—Ä—å–∏—Ä–æ–≤–∞–Ω–∏—è (dot-notation)
        values: –°–ø–∏—Å–æ–∫ –∑–Ω–∞—á–µ–Ω–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä–∞
        seeds: –°–ø–∏—Å–æ–∫ random seeds
        metrics_to_track: –°–ø–∏—Å–æ–∫ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
    """
    if metrics_to_track is None:
        metrics_to_track = ['val_loss']
    
    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –ë–∞–∑–æ–≤—ã–π –ö–æ–Ω—Ñ–∏–≥
    base_cfg = load_config("configs/config.yaml")

    print(f"\nüöÄ STARTING HYDRA EXPERIMENT: Varying '{param_to_vary}'")
    print(f"üìä Tracking metrics: {metrics_to_track}")

    logger = ExperimentLogger(
        output_dir=PATHS["output"], 
        experiment_name=f"exp_{param_to_vary.replace('.', '_')}", 
        base_config=base_cfg,
        param_name=param_to_vary
    )

    for val in values:
        for seed in seeds:
            current_cfg = base_cfg.copy()
            
            update_config_by_path(current_cfg, param_to_vary, val)
            
            print(f"üëâ Running: {param_to_vary}={val}, seed={seed}")

            try:
                result = run_single_trial(
                    current_cfg, 
                    seed, 
                    param_info={"name": param_to_vary, "value": val},
                    metrics_to_track=metrics_to_track
                )

                # 3. –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ (–¥–æ–ø–∏—Å—ã–≤–∞–µ—Ç –≤ —Ç–æ—Ç –∂–µ —Ñ–∞–π–ª)
                logger.log_result(result)
                
                # –í—ã–≤–æ–¥–∏–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                best_val = result.get('best_val_loss', result.get('min_val_loss'))
                if best_val is not None:
                    print(f"   ‚úÖ Best val_loss: {best_val:.6f}")
                
                # –í—ã–≤–æ–¥–∏–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                for metric in metrics_to_track:
                    if metric != 'val_loss':
                        best = result.get(f'best_{metric}')
                        if best is not None:
                            print(f"   üìà Best {metric}: {best:.6f}")

            except Exception as e:
                print(f"   ‚ùå Error: {e}")
                logger.log_result({
                    "param_value": val,
                    "seed": seed,
                    "error": str(e)
                })


if __name__ == "__main__":
    
    PARAM_NAME = "network.encoder.num_layers"  # TODO: enter your value, e.g. "network.encoder.out_channels"
    PARAM_VALUES =  [1, 2 ,3 ]  # TODO: enter values, e.g. [32, 64, 128]
    SEEDS = [42, 51]
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è: val_loss –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é  
    # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å: 'rank_me', 'isotropy', 'uniformity', 'silhouette' –∏ –¥—Ä.
    METRICS_TO_TRACK = ['val_loss']
    
    PATHS = {
        "stats": "/home/eugen/Desktop/CodeWork/Projects/Diplom/notebooks/GIT_Graph_refactor/data/stats/",
        "output": "/home/eugen/Desktop/CodeWork/Projects/Diplom/notebooks/GIT_Graph_refactor/exp/results/"  # TODO: enter dir for save, e.g. "./exp_results/"
    }
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–¥–∞–Ω—ã
    assert PARAM_NAME is not None, "Please set PARAM_NAME"
    assert PARAM_VALUES is not None, "Please set PARAM_VALUES"
    assert PATHS["output"] is not None, "Please set PATHS['output']"
    
    run_experiment_grid(PARAM_NAME, PARAM_VALUES, SEEDS, metrics_to_track=METRICS_TO_TRACK)